{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is used from the course materials and reduces the size of the dataframe for calculating an optimal r_sqared value:\n",
    "\n",
    "def find_optimal_lm_mod(X, y, cutoffs, test_size = .30, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "\n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "\n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        lm_model = LinearRegression(normalize=True)\n",
    "        lm_model.fit(X_train, y_train)\n",
    "        y_test_preds = lm_model.predict(X_test)\n",
    "        y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "\n",
    "    best_cutoff = max(results, key=results.get)\n",
    "\n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size)\n",
    "\n",
    "    #fit the model\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "\n",
    "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed_listings = pd.read_csv('./resource/detailed_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listing_cleaned = df_detailed_listings.drop(columns=['listing_url', 'scrape_id', 'last_scraped', 'source', 'picture_url', 'host_url', \\\n",
    "#     'host_thumbnail_url', 'host_picture_url', 'latitude', 'longitude', 'calendar_updated', 'calendar_last_scraped', 'license'], \n",
    "#     axis=1).copy()\n",
    "# df_listing_cleaned = df_listing_cleaned.dropna(axis=1, how='all')\n",
    "# df_listing_cleaned['price'] = df_listing_cleaned.price.str[1:].str.replace(',','').str.split('.').str[0].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to train a model to predict the price of a listing. Therefor we need to prepare the data further:\n",
    "\n",
    "1) Drop all features which doesn't contain useful data for our model, such as URLs, dates and coordinates.\n",
    "2) Drop all entries with missing values for the respondent, in this case the price.\n",
    "3) If there are missing values for numerical features we fill them with the mean.\n",
    "4) For the categorical values we need to implement dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all features which doesn't contain useful data for our model, such as URLs, dates and personal information.\n",
    "df = df_detailed_listings.drop(columns=['id', 'host_id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'picture_url', 'host_url', \\\n",
    "    'host_thumbnail_url', 'host_picture_url', 'latitude', 'name', 'host_about', 'description', 'neighborhood_overview', 'host_name', 'longitude', 'calendar_updated', 'calendar_last_scraped', 'license'], \n",
    "    axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the price column and change it into a usable integer datatype:\n",
    "df['price'] = df.price.str[1:].str.replace(',','').str.split('.').str[0].astype(int)\n",
    "\n",
    "# drop all entries with missing values for the respondent, in this case the price.\n",
    "df = df.dropna(subset=['price'], axis=0, how='all').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{int64: ['accommodates', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'], float64: ['host_listings_count', 'host_total_listings_count', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month'], object: ['host_since', 'host_location', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_neighbourhood', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'bathrooms_text', 'amenities', 'has_availability', 'first_review', 'last_review', 'instant_bookable']}"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features in the dataset: \n",
      "['host_listings_count', 'host_total_listings_count', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month']\n",
      "Categorical features in the dataset: \n",
      "['host_since', 'host_location', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_neighbourhood', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'bathrooms_text', 'amenities', 'has_availability', 'first_review', 'last_review', 'instant_bookable']\n"
     ]
    }
   ],
   "source": [
    "print(f'Numerical features in the dataset: \\n{list(df.select_dtypes([\"int\",\"float\"]).columns)}')\n",
    "print(f'Categorical features in the dataset: \\n{list(df.select_dtypes(\"object\").columns)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fill in the missing data. First for the numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['host_since', 'host_location', 'host_response_time',\n",
       "       'host_response_rate', 'host_acceptance_rate', 'host_is_superhost',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_has_profile_pic',\n",
       "       'host_identity_verified', 'neighbourhood', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'first_review', 'last_review',\n",
       "       'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill in all missing values regarding the numeric features we loop through them \n",
    "# and use the fillna method to fill them with the mean of the column\n",
    "num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "for col in num_vars:\n",
    "    df[col].fillna((df[col].mean()), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling all missing values for the numeric columns we take a look on the remaining missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['host_since', 'host_location', 'host_response_time',\n",
       "       'host_response_rate', 'host_acceptance_rate', 'host_is_superhost',\n",
       "       'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified',\n",
       "       'neighbourhood', 'bathrooms', 'bathrooms_text', 'first_review',\n",
       "       'last_review'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15717 entries, 0 to 15716\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   host_since              15697 non-null  object \n",
      " 1   host_location           13605 non-null  object \n",
      " 2   host_response_time      7981 non-null   object \n",
      " 3   host_response_rate      7981 non-null   object \n",
      " 4   host_acceptance_rate    8838 non-null   object \n",
      " 5   host_is_superhost       15713 non-null  object \n",
      " 6   host_neighbourhood      9303 non-null   object \n",
      " 7   host_has_profile_pic    15697 non-null  object \n",
      " 8   host_identity_verified  15697 non-null  object \n",
      " 9   neighbourhood           8188 non-null   object \n",
      " 10  bathrooms               0 non-null      float64\n",
      " 11  bathrooms_text          15703 non-null  object \n",
      " 12  first_review            12755 non-null  object \n",
      " 13  last_review             12755 non-null  object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_NaN = df.loc[:, df.columns[df.isna().any()]].copy() # type: ignore\n",
    "df_NaN.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the 'bathrooms' column contains only missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN.bathrooms.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['bathrooms']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15717 entries, 0 to 15716\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   host_since              15697 non-null  object\n",
      " 1   host_location           13605 non-null  object\n",
      " 2   host_response_time      7981 non-null   object\n",
      " 3   host_response_rate      7981 non-null   object\n",
      " 4   host_acceptance_rate    8838 non-null   object\n",
      " 5   host_is_superhost       15713 non-null  object\n",
      " 6   host_neighbourhood      9303 non-null   object\n",
      " 7   host_has_profile_pic    15697 non-null  object\n",
      " 8   host_identity_verified  15697 non-null  object\n",
      " 9   neighbourhood           8188 non-null   object\n",
      " 10  bathrooms_text          15703 non-null  object\n",
      " 11  first_review            12755 non-null  object\n",
      " 12  last_review             12755 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_NaN = df.loc[:, df.columns[df.isna().any()]].copy() # type: ignore\n",
    "df_NaN.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_location</th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_neighbourhood</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>first_review</th>\n",
       "      <th>last_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-10-19</td>\n",
       "      <td>Coledale, Australia</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>40%</td>\n",
       "      <td>f</td>\n",
       "      <td>Prenzlauer Berg</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>2009-06-20</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-08-25</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>within a day</td>\n",
       "      <td>75%</td>\n",
       "      <td>0%</td>\n",
       "      <td>f</td>\n",
       "      <td>Prenzlauer Berg</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>2.5 baths</td>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-11-14</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>87%</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>2 baths</td>\n",
       "      <td>2010-11-30</td>\n",
       "      <td>2022-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-11-18</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>within a day</td>\n",
       "      <td>90%</td>\n",
       "      <td>9%</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>2010-06-29</td>\n",
       "      <td>2021-06-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-11-08</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>within a day</td>\n",
       "      <td>100%</td>\n",
       "      <td>90%</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5 baths</td>\n",
       "      <td>2011-09-06</td>\n",
       "      <td>2022-10-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_since        host_location  host_response_time host_response_rate  \\\n",
       "0  2008-10-19  Coledale, Australia  within a few hours               100%   \n",
       "1  2009-08-25      Berlin, Germany        within a day                75%   \n",
       "2  2009-11-14      Berlin, Germany  within a few hours               100%   \n",
       "3  2009-11-18      Berlin, Germany        within a day                90%   \n",
       "4  2010-11-08      Berlin, Germany        within a day               100%   \n",
       "\n",
       "  host_acceptance_rate host_is_superhost host_neighbourhood  \\\n",
       "0                  40%                 f    Prenzlauer Berg   \n",
       "1                   0%                 f    Prenzlauer Berg   \n",
       "2                  87%                 t    Prenzlauer Berg   \n",
       "3                   9%                 t    Prenzlauer Berg   \n",
       "4                  90%                 t    Prenzlauer Berg   \n",
       "\n",
       "  host_has_profile_pic host_identity_verified    neighbourhood bathrooms_text  \\\n",
       "0                    t                      t  Berlin, Germany         1 bath   \n",
       "1                    t                      t  Berlin, Germany      2.5 baths   \n",
       "2                    t                      t  Berlin, Germany        2 baths   \n",
       "3                    t                      t              NaN         1 bath   \n",
       "4                    t                      t              NaN      1.5 baths   \n",
       "\n",
       "  first_review last_review  \n",
       "0   2009-06-20  2021-01-01  \n",
       "1   2015-08-09  2020-01-04  \n",
       "2   2010-11-30  2022-10-23  \n",
       "3   2010-06-29  2021-06-21  \n",
       "4   2011-09-06  2022-10-13  "
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating the other columns containing missing values, we further drop the columns containing dates, because they're not that important for our model in regard of predicting the price of an apartment. Also the hosts location doesn't seem that relevant either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['host_since','host_location','first_review','last_review','host_neighbourhood']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>bathrooms_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>40%</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>within a day</td>\n",
       "      <td>75%</td>\n",
       "      <td>0%</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>2.5 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>87%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>2 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>within a day</td>\n",
       "      <td>90%</td>\n",
       "      <td>9%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>within a day</td>\n",
       "      <td>100%</td>\n",
       "      <td>90%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5 baths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_response_time host_response_rate host_acceptance_rate  \\\n",
       "0  within a few hours               100%                  40%   \n",
       "1        within a day                75%                   0%   \n",
       "2  within a few hours               100%                  87%   \n",
       "3        within a day                90%                   9%   \n",
       "4        within a day               100%                  90%   \n",
       "\n",
       "  host_is_superhost host_has_profile_pic host_identity_verified  \\\n",
       "0                 f                    t                      t   \n",
       "1                 f                    t                      t   \n",
       "2                 t                    t                      t   \n",
       "3                 t                    t                      t   \n",
       "4                 t                    t                      t   \n",
       "\n",
       "     neighbourhood bathrooms_text  \n",
       "0  Berlin, Germany         1 bath  \n",
       "1  Berlin, Germany      2.5 baths  \n",
       "2  Berlin, Germany        2 baths  \n",
       "3              NaN         1 bath  \n",
       "4              NaN      1.5 baths  "
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN = df.loc[:, df.columns[df.isna().any()]].copy() # type: ignore\n",
    "df_NaN.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 'neighbourhood' column contains pretty messy data (shown in the next cell) and we already have a cleansed version of ot, we can drop it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Berlin, Germany' nan 'Friedrichshain, Berlin, Germany'\n",
      " 'Berlin-Wedding, Berlin, Germany' 'Berlin Neukölln , Berlin, Germany'\n",
      " 'Berlin-Mitte, Berlin, Germany' 'Berlín, Berlin, Germany'\n",
      " 'Берлин, Berlin, Germany' 'Berlin, Mitte, Berlin, Germany'\n",
      " 'Berlin - Mitte, Germany' 'Berlin - Schöneberg, Berlin, Germany'\n",
      " 'Berlin- Charlottenburg, Berlin, Germany'\n",
      " 'Berlin, schmargendorf, Germany' 'Mitte/Tiergarten, Berlin, Germany'\n",
      " 'berlin, Berlin, Germany' 'Berlin-Kreuzberg, Berlin, Germany'\n",
      " 'Berlin Friedrichshain, Berlin, Germany' 'Berlin, Zehlendorf, Germany'\n",
      " 'Weissenhoher Strasse 14, Berlin, Germany' 'Berlin, DE, Germany'\n",
      " 'Berlin, neukoelln, Germany' 'Berlin, Be, Germany'\n",
      " 'Berlin, Berlin, DE, Berlin, Germany' 'Hoppegarten, Brandenburg, Germany'\n",
      " 'Germany' 'Berlin , Germany' 'Berlin, SN, Germany'\n",
      " 'Weissensee, Berlin, Germany' 'Potsdam, Brandenburg, Germany'\n",
      " 'Berlin-Bohnsdorf, Germany' 'Berlin, Berlín, Germany' '柏林, Germany'\n",
      " 'Berlin Prenzlauer Berg , Berlin, Germany' 'Mitte, Berlin, Germany'\n",
      " 'Schöneiche bei Berlin, Brandenburg, Germany' 'Berlin, Spandau, Germany']\n"
     ]
    }
   ],
   "source": [
    "print(df_NaN.neighbourhood.unique())\n",
    "df = df.drop(columns=['neighbourhood']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>bathrooms_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>40%</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>within a day</td>\n",
       "      <td>75%</td>\n",
       "      <td>0%</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>2.5 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>87%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>2 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>within a day</td>\n",
       "      <td>90%</td>\n",
       "      <td>9%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>within a day</td>\n",
       "      <td>100%</td>\n",
       "      <td>90%</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1.5 baths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_response_time host_response_rate host_acceptance_rate  \\\n",
       "0  within a few hours               100%                  40%   \n",
       "1        within a day                75%                   0%   \n",
       "2  within a few hours               100%                  87%   \n",
       "3        within a day                90%                   9%   \n",
       "4        within a day               100%                  90%   \n",
       "\n",
       "  host_is_superhost host_has_profile_pic host_identity_verified bathrooms_text  \n",
       "0                 f                    t                      t         1 bath  \n",
       "1                 f                    t                      t      2.5 baths  \n",
       "2                 t                    t                      t        2 baths  \n",
       "3                 t                    t                      t         1 bath  \n",
       "4                 t                    t                      t      1.5 baths  "
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN = df.loc[:, df.columns[df.isna().any()]].copy() # type: ignore\n",
    "df_NaN.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the columns containing missing data seem to be interesting and should be included into the model, if they can. Lets see how many values ara actually missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_response_time           host_response_time: 49.22% missing values\n",
       "host_response_rate           host_response_rate: 49.22% missing values\n",
       "host_acceptance_rate       host_acceptance_rate: 43.77% missing values\n",
       "host_is_superhost              host_is_superhost: 0.03% missing values\n",
       "host_has_profile_pic        host_has_profile_pic: 0.13% missing values\n",
       "host_identity_verified    host_identity_verified: 0.13% missing values\n",
       "bathrooms_text                    bathrooms_text: 0.09% missing values\n",
       "dtype: object"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN.apply(lambda col: f\"{col.name}: {round(col.isnull().sum() / len(col) * 100, 2)}% missing values\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more than 40% missing values in the three columns 'host_response_time', 'host_response_rate' and 'host_acceptance_rate' we drop these as well, since there is just to much data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['host_response_time','host_response_rate','host_acceptance_rate']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>bathrooms_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>2.5 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>2 baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1 bath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1.5 baths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  host_is_superhost host_has_profile_pic host_identity_verified bathrooms_text\n",
       "0                 f                    t                      t         1 bath\n",
       "1                 f                    t                      t      2.5 baths\n",
       "2                 t                    t                      t        2 baths\n",
       "3                 t                    t                      t         1 bath\n",
       "4                 t                    t                      t      1.5 baths"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NaN = df.loc[:, df.columns[df.isna().any()]].copy() # type: ignore\n",
    "df_NaN.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest we jus drop the rows containing the missing data, since the amount is neglectable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows droped: 34\n"
     ]
    }
   ],
   "source": [
    "num_rows_before_drop = df.shape[0]\n",
    "df = df.dropna(subset=['host_is_superhost','host_has_profile_pic','host_identity_verified','bathrooms_text'], axis=0).copy()\n",
    "num_rows_after_drop = df.shape[0]\n",
    "\n",
    "print(f'Number of rows droped: {num_rows_before_drop - num_rows_after_drop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_verifications</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>neighbourhood_group_cleansed</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>bathrooms_text</th>\n",
       "      <th>amenities</th>\n",
       "      <th>has_availability</th>\n",
       "      <th>instant_bookable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg Südwest</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>Entire rental unit</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>[\"Smoke alarm\", \"Kitchen\", \"Private hot tub\", ...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg Südwest</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>Entire rental unit</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2.5 baths</td>\n",
       "      <td>[\"Smoke alarm\", \"Bed linens\", \"Oven\", \"Kitchen...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg Nordwest</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>Entire rental unit</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2 baths</td>\n",
       "      <td>[\"Smoke alarm\", \"Bed linens\", \"Oven\", \"Kitchen...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t</td>\n",
       "      <td>['email', 'phone', 'work_email']</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg Nordwest</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>Entire rental unit</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>1 bath</td>\n",
       "      <td>[\"Smoke alarm\", \"Dishes and silverware\", \"Host...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t</td>\n",
       "      <td>['email', 'phone']</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>Prenzlauer Berg Süd</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>Entire rental unit</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>1.5 baths</td>\n",
       "      <td>[\"Bed linens\", \"TV with standard cable\", \"Dish...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  host_is_superhost                host_verifications host_has_profile_pic  \\\n",
       "0                 f                ['email', 'phone']                    t   \n",
       "1                 f                ['email', 'phone']                    t   \n",
       "2                 t                ['email', 'phone']                    t   \n",
       "3                 t  ['email', 'phone', 'work_email']                    t   \n",
       "4                 t                ['email', 'phone']                    t   \n",
       "\n",
       "  host_identity_verified    neighbourhood_cleansed  \\\n",
       "0                      t   Prenzlauer Berg Südwest   \n",
       "1                      t   Prenzlauer Berg Südwest   \n",
       "2                      t  Prenzlauer Berg Nordwest   \n",
       "3                      t  Prenzlauer Berg Nordwest   \n",
       "4                      t       Prenzlauer Berg Süd   \n",
       "\n",
       "  neighbourhood_group_cleansed       property_type        room_type  \\\n",
       "0                       Pankow  Entire rental unit  Entire home/apt   \n",
       "1                       Pankow  Entire rental unit  Entire home/apt   \n",
       "2                       Pankow  Entire rental unit  Entire home/apt   \n",
       "3                       Pankow  Entire rental unit  Entire home/apt   \n",
       "4                       Pankow  Entire rental unit  Entire home/apt   \n",
       "\n",
       "  bathrooms_text                                          amenities  \\\n",
       "0         1 bath  [\"Smoke alarm\", \"Kitchen\", \"Private hot tub\", ...   \n",
       "1      2.5 baths  [\"Smoke alarm\", \"Bed linens\", \"Oven\", \"Kitchen...   \n",
       "2        2 baths  [\"Smoke alarm\", \"Bed linens\", \"Oven\", \"Kitchen...   \n",
       "3         1 bath  [\"Smoke alarm\", \"Dishes and silverware\", \"Host...   \n",
       "4      1.5 baths  [\"Bed linens\", \"TV with standard cable\", \"Dish...   \n",
       "\n",
       "  has_availability instant_bookable  \n",
       "0                t                f  \n",
       "1                t                f  \n",
       "2                t                f  \n",
       "3                t                f  \n",
       "4                t                f  "
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'host_verifications' and 'amenities' are actual strings of lists, so we need to convert them to actual lists containing separate strings to be able to get dummy variables for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_verifications'] = df['host_verifications'].apply(eval).copy()\n",
    "df['amenities'] = df['amenities'].apply(eval).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After continuing with this dataframe, substituting the other categorical columns also with dummy variables, the model has shown to be significantly overfitted with a r_squared value of -2.484865441020741e+27. To combat overfitting we reduce the number of features by selecting those seeming relevant and discarding the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['amenities']).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can get the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/xhk5qpfs7jjgjnyf_wvh1ktm0000gn/T/ipykernel_46480/2249114786.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  dummy_cols_hv = pd.get_dummies(df['host_verifications'].apply(pd.Series).stack()).sum(level=0)\n",
      "/var/folders/qd/xhk5qpfs7jjgjnyf_wvh1ktm0000gn/T/ipykernel_46480/2249114786.py:1: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  dummy_cols_hv = pd.get_dummies(df['host_verifications'].apply(pd.Series).stack()).sum(level=0)\n"
     ]
    }
   ],
   "source": [
    "dummy_cols_hv = pd.get_dummies(df['host_verifications'].apply(pd.Series).stack()).sum(level=0)\n",
    "# dummy_cols_am = pd.get_dummies(df['amenities'].apply(pd.Series).stack()).sum(level=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And append them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, dummy_cols_hv], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df, dummy_cols_am], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can drop the original columns containing the lists of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['host_verifications']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email', 'phone', 'photographer', 'work_email']"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After appending the dummy variables for the both columns containing lists there are several missing values back in our dataframe. Since the information shouldn't be to influential (since it's mostly different kinds of soap) we just give them the value 0 and treat them as they weren't there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to get the dummy variable of all other categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                 2\n",
       "host_has_profile_pic              2\n",
       "host_identity_verified            2\n",
       "neighbourhood_cleansed          138\n",
       "neighbourhood_group_cleansed     12\n",
       "property_type                    66\n",
       "room_type                         4\n",
       "bathrooms_text                   27\n",
       "has_availability                  2\n",
       "instant_bookable                  2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "catvars = df.select_dtypes(include=['object']).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=catvars, drop_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prepping the data we split the dataset into the X Matrix and the respondent y, and further into sub-datasets used for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.price.isnull().sum())\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price']).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical features: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15683, 283)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of categorical features: {len(X.select_dtypes(include='object').columns.to_list())}\")\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute all categorical features with dummy vaiables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = X.select_dtypes(include=['object']).copy().columns\n",
    "for var in  cat_vars:\n",
    "    # for each cat add dummy var, drop original column\n",
    "    X = pd.concat([X.drop(var, axis=1), pd.get_dummies(X[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview over the new dimensions of the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical features: 0\n",
      "Dimensions of the dataframe: \n",
      "Features: 283 - Rows: 15683\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of categorical features: {len(X.select_dtypes(include='object').columns.to_list())}\")\n",
    "print(f\"Dimensions of the dataframe: \\nFeatures: {X.shape[1]} - Rows: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4705, 283) (10978, 283) (10978,) (4705,) (15683, 283) (15683,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_train.shape, y_train.shape, y_test.shape, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(normalize=True)"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.9612077311497045e+24 0.023684199646298087\n"
     ]
    }
   ],
   "source": [
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "print(test_score, train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/danielhutcheson/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#cutoffs here pertains to the number of missing values allowed in the used columns.\n",
    "#Therefore, lower values for the cutoff provides more predictors in the model.\n",
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 30, 20, 10, 5]\n",
    "\n",
    "r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = find_optimal_lm_mod(X, y, cutoffs, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4705, 42) (10978, 42) (10978,) (4705,) (15683, 283) (15683,)\n",
      "False False False\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_train.shape, y_train.shape, y_test.shape, X.shape, y.shape)\n",
    "print(X.isna().any().any(), X_train.isna().any().any(), X_test.isna().any().any())\n",
    "print(y.isna().any().any(), y_train.isna().any().any(), y_test.isna().any().any())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9c059572ed3138442ca94ddfc11038f56d8174bb8c1e572332b95c07613972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
